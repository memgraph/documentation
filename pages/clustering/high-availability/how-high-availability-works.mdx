---
title: How high availability works
description: Learn about the underlying implementation and theoretical concepts behind Memgraph's high availability.
---

import { Callout } from 'nextra/components'
import { Steps } from 'nextra/components'
import {CommunityLinks} from '/components/social-card/CommunityLinks'


# How high availability Works

<Callout type="info">

This guide is a continuation of [how replication works](/clustering/concepts/how-replication-works).
We recommend reading it first, before moving to the high availability concepts.

</Callout>

A cluster is considered highly available if, at any point, there is some instance that can respond to a user query.
Our high availability relies on replication and automatic failover. The cluster consists of:
- The MAIN instance on which the user can execute write queries
- REPLICA instances that can only respond to read queries
- COORDINATOR instances that manage the cluster state.

MAIN and REPLICA instances can also be called **data instances** in this context, as each data instance can 
interchange the role of a MAIN or replica during the course of cluster lifecycle. Data instances are the ones that
are holding the graph data which is being replicated.

**The coordinator instances are neccessary to orchestrate the data instances and ensure that there is always one
main instance in the cluster.** Coordinator instances are much smaller than the data instances, since the sole
purpose of them is to manage the cluster.

## How is high availability achieved?

**For achieving high availability, Memgraph uses Raft consensus protocol**, which is very similar to Paxos in terms of performance and fault-tolerance but with 
a significant advantage that it is much easier to understand. It's important to say that Raft isn't a
Byzantine fault-tolerant algorithm. You can learn more about Raft in the paper [In Search of an Understandable Consensus Algorithm](https://raft.github.io/raft.pdf).
**As a design decision, Memgraph uses an industry-proven library [NuRaft](https://github.com/eBay/NuRaft) for the implementation of the Raft
protocol.**

Typical Memgraph's highly available cluster consists of:
- 3 data instances (1 MAIN and 2 REPLICAs)
- 3 coordinator instances (1 leader and 2 followers)

![](/pages/clustering/high-availability/typical_ha_cluster.png)

The constraint for number coordinators is only that it needs to be an **odd number of them, greater than 1** (3, 5, 7, ...).
Users can create more than 3 coordinators, but the replication factor (RF) of 3 is a de facto standard in distributed databases.

Minimal setup for data instances is 1 MAIN and 1 REPLICA. If the MAIN fails, the coordinators will elect the REPLICA as the new main
instance.

![](/pages/clustering/high-availability/minimal_ha_cluster.png)

<Callout type="info">

The Raft consensus algorithm ensures that all nodes in a distributed system
agree on a single source of truth, even in the presence of failures, by electing
a leader to manage a replicated log. It simplifies the management of the
replicated log across the cluster, providing a way to achieve consistency and
coordination in a fault-tolerant manner. **Users are advised to use an odd number of coordinator instances**
since Raft, as a consensus algorithm, works by forming a majority in the decision making.

</Callout>

One coordinator instance is the leader whose job is to always ensure there is exactly one MAIN, or one writeable instance.
Other two coordinator instances are called FOLLOWER coordinators.
The follower coordinators replicate changes the leader coordinator did in its own Raft log.
Operations saved into the Raft log are those that are related to cluster management. **With this setup, coordinators are also
highly available, therefore making the whole cluster highly available.**

### Data instance implementation

The data instance is your usual Memgraph standalone instance, with one key flag added:
- `--management-port` - used to get the **health state** of the data instance from the leader coordinator

### Coordinator instance implementation

The coordinator is a small orchestration instance which is shipped in the same Memgraph binary as the data instance itself. For the system
to be aware that its role is COORDINATOR, the user needs to specify three flags:
- `--coordinator-id` - serves as a **unique identifier** of the coordinator
- `--coordinator-port` - used for **synchronization and log replication** between coordinators
- `--coordinator-hostname` - used on followers to ping the leader on the correct IP address (or FQDN/DNS name)
- `--management-port` - used to get the **health state** of the respective coordinator instance from the leader coordinator

<Callout>
The COORDINATOR instance is a very restricted instance, and it will not respond to any queries that are not related to management of the cluster.
That means, you can not run any data queries on the coordinator directly (we will talk more about routing data queries in the next sections).
</Callout>

#### How heavy are coordinator instances?

When deploying coordinators to servers, you can use the instance of almost any size. Instances of 4GiB or 8GiB will suffice since coordinators'
job mainly involves network communication and storing Raft metadata. Coordinators and data instances can in theory be deployed on
same servers (pairwise) but from the availability perspective, it is better to separate them physically. 

### RPC messages sent through the cluster

<Callout type="info">
RPC (Remote Procedure Call) is a protocol for executing functions on a remote
system. RPC enables direct communication in distributed systems and is crucial
for replication and high availability tasks. 
</Callout>

#### Types of RPC messages

Memgraph is ensuring a consistent state of the cluster using RPC messages. Below is a full list of RPC messages and their purpose.
Each 

- `ShowInstancesRpc` - Sent by the follower coordinator to the leader coordinator if the user executed `SHOW INSTANCES` through the follower
  coordinator.
- `DemoteMainToReplicaRpc` - Sent by the leader coordinator to the old MAIN in order to demote it to REPLICA.
- `PromoteToMainRpc` - Sent by the leader coordinator to a REPLICA in order to promote it to MAIN.
- `RegisterReplicaOnMainRpc` - Sent by the leader coordinator to MAIN in order to register a REPLICA on MAIN.
- `UnregisterReplicaRpc` - Sent by the leader coordinator to MAIN in order to register a REPLICA on MAIN.
- `EnableWritingOnMainRpc` - Sent by the leader coordinator to MAIN to enable writing on that MAIN.
- `GetDatabaseHistoriesRpc` - Sent by the leader coordinator to all REPLICA instances in order to select new MAIN
  during the failover process.
- `StateCheckRpc` - Sent by the leader coordinator to all data instances for liveness check.
- `SwapMainUUIDRpc` - Sent by the leader coordinator to REPLICA instances to set which MAIN to listen to.
- `FrequentHeartbeatRpc` - Sent by the MAIN instance to a REPLICA for liveness check.
- `HeartbeatRpc` - Sent by the MAIN instance to a REPLICA instance for timestamp/epoch/transaction/commit information.
- `SystemRecoveryRpc` - Sent by the MAIN instance to a REPLICA instance for system replication queries (auth, multi-tenancy)
  and other non-graph information.
- `PrepareCommitRpc` - Sent by the MAIN instance to the REPLICA instances, either as a first phase in `STRICT_SYNC` mode, or as the only phase in other
  replication modes. Used to send the delta stream during the execution of a write query.
- `FinalizeCommitRpc` - Sent by the MAIN instance to the REPLICA instances in `STRICT_SYNC` replication mode when all the REPLICAs have
  answered that they're ready to commit.
- `SnapshotRpc` - Sent by the MAIN to a REPLICA for snapshot recovery to catch-up with the MAIN instance.
- `WalFilesRpc` - Sent by the MAIN to a REPLICA for WAL recovery to catch-up with the MAIN instance.
- `CurrentWalRpc` - Sent by the MAIN to a REPLICA for current/latest/unfinished WAL file recovery to catch-up with the MAIN instance.

#### RPC timeouts

##### Default RPC timeouts

For the majority of RPC messages, Memgraph uses a default timeout of 10s. This is to ensure that when sending a RPC request, the client 
will not block indefinitely before receiving a response if the communication between the client and the server is broken. The list of RPC messages
for which the timeout is used is the following: 

##### RPC timeout during replication of data

For RPC messages which are sending the variable number of storage deltas — `PrepareCommitRpc`, `CurrentWalRpc`, and
`WalFilesRpc` — it is not practical to set a strict execution timeout, as the
processing time on the replica side is directly proportional to the number of
deltas being transferred. To handle this, the replica sends periodic progress
updates to the main instance after processing every 100,000 deltas. Since
processing 100,000 deltas is expected to take a relatively consistent amount of
time, we can enforce a timeout based on this interval. The default timeout for
these RPC messages is 30 seconds, though in practice, processing 100,000 deltas
typically takes less than 3 seconds.

##### RPC timeout during replica recovery

`SnapshotRpc` is also a replication-related RPC message, but its execution time
is tracked a bit differently from RPC messages shipping deltas. The replica sends an update to the main instance after
completing 1,000,000 units of work. The work units are assigned as follows:

- Processing nodes, edges, or indexed entities (label index, label-property index,
  edge type index, edge type property index) = 1 unit
- Processing a node inside a point or text index = 10 units
- Processing a node inside a vector index (most computationally expensive) =
  1,000 units

With this unit-based tracking system, the REPLICA is expected to report progress
approximately every 2–3 seconds. Given this, a timeout of 60 seconds is set to avoid
unnecessary network instability while ensuring responsiveness. On every report of the progress
by the REPLICA, the timeout of 60 seconds will be restarted, as the state of recovery is considered to be stable.

Except for timeouts on read and write operations, Memgraph also has a timeout of 5s
for sockets when establishing a connection. Such a timeout helps in having a low p99
latencies when using the RPC stack, which manifests for users as smooth and predictable
network communication between instances.

In the table below, we have the full outline of the RPC messages that are sent in the cluster to ensure high availability, with timeouts. 

| RPC message request      | source      | target         | timeout          |
|--------------------------|-------------|----------------| -----------------|
| ShowInstancesReq         | Coordinator | Coordinator    |                  |
| DemoteMainToReplicaReq   | Coordinator | Data instance  |                  |
| PromoteToMainReq         | Coordinator | Data instance  |                  |
| RegisterReplicaOnMainReq | Coordinator | Data instance  |                  |
| UnregisterReplicaReq     | Coordinator | Data instance  |                  | 
| EnableWritingOnMainReq   | Coordinator | Data instance  |                  |
| GetDatabaseHistoriesReq  | Coordinator | Data instance  |                  |
| StateCheckReq            | Coordinator | Data instance  | 5s               |
| SwapMainUUIDReq          | Coordinator | Data instance  |                  |
| FrequentHeartbeatReq     | Main        | Replica        | 5s               |
| HeartbeatReq             | Main        | Replica        |                  |
| SystemRecoveryReq        | Main        | Replica        | 5s               |
| PrepareCommitRpc         | Main        | Replica        | proportional     |
| FinalizeCommitReq        | Main        | Replica        | 10s              |
| SnapshotRpc              | Main        | Replica        | proportional     |
| WalFilesRpc              | Main        | Replica        | proportional     |
| CurrentWalRpc            | Main        | Replica        | proportional     |

## Automatic failover

### High level description

Automatic failover can be performed based on health checks that the leader coordinator performs to the data instances. A data instance is considered
to be *alive* if it responds to health checks. If it doesn't respond to health checks, it is considered to be *down*.

**If the instance which was down was a REPLICA instance**, it will always rejoin the cluster as a REPLICA.

**If the instance which was down was a MAIN instance**, the coordinator might perform a failover procedure to choose the new MAIN from the alive REPLICAs.

From alive replicas coordinator chooses a new potential main and writes a log to the Raft storage about the new main.
On the next leader's ping to the instance, it will send to the instance an RPC request (`PromoteToMainReq`)
to the new main, which is still in replica state, to promote itself to the main instance with info about other replicas
to which it will replicate data. Once that request succeeds, the new main can start replication to the other instances
and accept write queries.


### Instance health checks

Every second (set by the flag --instance-health-check-frequency-sec`), the coordinator contacts each instance.
The instance is not considered to be down unless the health check timeouts (set by the flag `--instance-down-timeout-sec`),
and the instance hasn't responded to the coordinator in the meantime.

For example, set `--instance-down-timeout-sec=5` and `--instance-health-check-frequency-sec=1` which will result in coordinator
contacting each instance every second (`StateCheckRpc`) and  the instance is considered dead after it doesn't respond 5 times (5 seconds / 1 second).
Usually, the health check reports instantly, and only in severe cases of network failure, it will timeout in 30 seconds.

Depending on the data instance role, we have 2 scenarios:
1. In case a **REPLICA instance doesn't respond to a health check**, the leader coordinator will try to contact it again on the next scheduled interval.
  **Replica instance will always rejoin the cluster as a replica.**

![](/pages/clustering/high-availability/replica-rejoining-cluster.png)

2. In case a **MAIN instance doesn't respond to a health check**, there are two options:
    - If it is down for less than `--instance-down-timeout-sec` interval, it will rejoin as MAIN because it is still considered alive. 
    - If it is down for more than `--instance-down-timeout-sec` interval, the failover procedure is initiated. Whether main will
      rejoin as main depends on the success of the failover procedure.
        - If the failover procedure succeeds, now old main will rejoin as REPLICA.
        - If failover doesn't succeed, MAIN will rejoin as MAIN once it comes back up.

![](/pages/clustering/high-availability/main-rejoining-cluster.png)

<Callout type="info">
You can check the [best practices](/clustering/high-availability/best-practices) if you want to see more about instance health check configuration.
</Callout>

### Choosing new MAIN from available REPLICAs


During failover, the coordinator must select a new main instance from available replicas, as some may be offline. The leader coordinator queries each live replica to 
retrieve the committed transaction count for every database.

<Callout>
*For every database* in this terminology, means if you're using [multi-tenancy](/database-management/multi-tenancy),
to isolate different graphs/databases under one instance. If you're not using multi-tenancy, the retrieved committed transaction
count will just be retrieved for the default *memgraph* database.
</Callout>

The selection algorithm prioritizes data recency using a two-phase approach:

1. **Database majority rule**: The coordinator identifies which replica has the highest committed transaction count for each database. The replica that leads in the most
databases becomes the preferred candidate.
2. **Total transaction tiebreaker**: If multiple replicas tie for leading the most databases, the coordinator sums each replica's committed transactions across all databases.
The replica with the highest total becomes the new main.

This approach ensures the new main instance has the most up-to-date data across the cluster while maintaining consistency guarantees.

### Old MAIN rejoining to the cluster

Once the old MAIN gets back up, the coordinator sends an RPC request to demote the old MAIN to REPLICA (`DemoteMainToReplicaReq`).
The coordinator tracks at all times which instance was the last main. 

The leader coordinator sends two RPC requests in the given order to demote old MAIN to REPLICA:
1. Demote MAIN to REPLICA RPC request
2. A request to store the machine UUID of the current MAIN, which the old MAIN, now acting as a REPLICA instance, must listen to.

### How REPLICA knows which MAIN to listen

Each replica has a UUID of MAIN it listens to. If a network partition happens where MAIN can talk to a REPLICA but the coordinator can't
talk to the MAIN, from the coordinator's point of view that MAIN is down. From REPLICA's point of view, the MAIN instance is still
alive. The coordinator will start the failover procedure, and we can end up with multiple MAINs where REPLICAs can listen to both
MAINs. To prevent such an issue, each REPLICA gets a new UUID that no current MAIN has. The coordinator generates the new UUID, 
which the new MAIN will get to use on its promotion to MAIN. 

If REPLICA was down at one point, MAIN could have changed. When REPLICA gets back up, coordinator will initiate an
RPC request (`SwapMainUUIDRpc` message), in order to set the UUID of the MAIN to which the REPLICA should listen to.

### Replication scenarios

#### Force sync of data

We have outlined above how Memgraph 
[selectes the most up-to-date alive instance](/clustering/concepts/how-high-availability-works#choosing-new-main-from-available-replicas).

However, there might be another instance which might have had a more up-to-date state, but was down at the time of the failover process.
**This scenario is called force sync of REPLICA instance.**

If a previously down instance had more up-to-date data but was unavailable
during failover, it will go through a specific recovery process upon rejoining
the cluster:
- The REPLICA will reset its storage.
- The REPLICA will receive all commits from the new main to
  synchronize its state.
- The REPLICA's old durability files will be preserved in a `.old` directory in
  `data_directory/snapshots` and `data_directory/wal` folders, allowing admins
  to manually recover data if needed. The `.old` directory is reused for subsequent
  recovery operations, meaning **only a single backup is maintained at any time**.

Depending on the replication mode used, there are different levels of data loss
that can happen upon the failover. With the default `SYNC` replication mode,
Memgraph prioritizes availability over strict consistency and can result in
a non-zero Recovery Point Objective (RPO), that is, the loss of committed data, because:
- The promoted main might not have received all commits from the previous main
  before the failure.
- This design ensures that the main remains writable for the maximum possible
  time.

With `ASYNC` replication mode, you also risk losing some data upon the failover because
main can freely continue commiting no matter the status of ASYNC replicas.

The `STRICT_SYNC` replication mode allows users experiencing a failover **without any data loss**
in all situations. It comes with reduced throughput because of the cost of running two-phase commit protocol.

<Callout>
Learn more about implications of [different replication modes](/clustering/concepts/how-replication-works/replication-modes).
</Callout>


## Actions on follower coordinators

From follower coordinators you can only execute `SHOW INSTANCES`. Registration of data instance, unregistration of data instances, demoting instance, setting instance to main and 
force resetting cluster state are all disabled. 


## Instances' restart

### Data instances' restart

Data instances can fail both as main and as replica. When an instance that was replica comes back, it will listen for
the MAIN instance based on the UUID provided from the coordinator (`SwapMainUUIDReq` message).
This should happen automatically when the coordinator's ping to the instance passes. When the main instance
comes back, any writing to the main instance will be forbidden until a ping from the coordinator passes
(`EnableWritingOnMainRpc` message).

### Coordinator instances restart

In case the coordinator instance dies and it is restarted, it will not lose any data from the RAFT log or RAFT snapshots,
since coordinator data is always backed-up by a durable storage.
We talk more about this in the [RAFT implementation](#raft-implementation).


## RAFT implementation

All NuRaft data is made durable by default. This includes all Raft logs, Raft snapshots and information about cluster connectivity. 
The details about the cluster connectivity are made durable since without that information, the coordinator can't rejoin the cluster on its restart.

Information about logs and snapshots is stored under one RocksDB instance in the `high_availability/raft_data/logs` directory stored under 
the top-level `--data-directory` folder. All the data stored there is recovered in case the coordinator restarts.

Data about other coordinators is recovered from the `high_availability/raft_data/network` directory stored under 
the top-level `--data-directory` folder. When the coordinator rejoins, it will reestablish the communication with other coordinators and receive updates from the current leader.

### First start 

On the first start of coordinators, each will store the current version of the `logs` and `network` durability store. From that point on,
each RAFT log that is sent to the coordinator is also stored on disk. For every new coordinator instance, the server config is updated. Logs are created 
for each user action and failover action. Snapshots are created every N (currently set to 5) logs. 


### Restart of coordinator 

In case of the coordinator's failure, on the restart, it will read information about other coordinators stored under `high_availability/raft_data/network` directory. 

From the `network` directory we will recover the server state before the coordinator stopped, including the current term, for whom the coordinator voted, and whether 
election timer is allowed.

It will also recover the following server config information: 
- other servers, including their endpoints, id, and auxiliary data
- ID of the previous log 
- ID of the current log 
- additional data needed by nuRaft

The following information will be recovered from a common RocksDB `logs` instance: 
- current version of `logs` durability store 
- snapshots found with `snapshot_id_` prefix in database:
  - coordinator cluster state - all data instances with their role (main or replica), all coordinator instances and UUID of main instance which replica is listening to 
  - last log idx
  - last log term
  - last cluster config
- logs found in the interval between the start index and the last log index 
  - data - each log holds data on what has changed since the last state
  - term - nuRAFT term 
  - log type - nuRAFT log type


### Handling of durability errors 

If snapshots are not correctly stored, the exception is thrown and left for the nuRAFT library to handle the issue. Logs can be missed and not stored since they are compacted and 
deleted every two snapshots and will be removed relatively fast.

Memgraph throws an error when failing to store cluster config, which is updated in the `high_availability/raft_data/network` folder. 
If this happens, it will happen only on the first cluster start when coordinators are connecting since 
coordinators are configured only once at the start of the whole cluster. This is a non-recoverable error since in case the coordinator rejoins the cluster and has 
the wrong state of other clusters, it can become a leader without being connected to other coordinators. 


## Recovering from errors

Distributed systems can fail in numerous ways. Memgraph processes are resilient to network
failures, omission faults and independent machine failures. Byzantine failures aren't tolerated since the Raft consensus protocol cannot deal with them either.

Recovery Time Objective (RTO) is an often used term for measuring the maximum tolerable length of time that an instance or cluster can be down.
Since every highly available Memgraph cluster has two types of instances, we need to analyze the failures of each separately.

Raft is a quorum-based protocol and it needs a majority of instances alive in order to stay functional. Hence, with just one coordinator instance down, RTO is 0 since
the cluster stays available. With 2+ coordinator instances down
(in a cluster with RF = 3), the RTO depends on the time needed for instances to come back.

Depending on the replica's replication mode, its failure can lead to different situations. If the replica was registered with STRICT_SYNC mode, then on its failure, writing
on main will be disabled. On the other hand, if replica was registered as ASYNC or SYNC, further writes on main are still allowed. In both cases, reads are still allowed from
main and other replicas.


The most important thing to analyze is what happens when main gets down. In that case, the leader coordinator uses 
user-controllable parameters related to the frequency of health checks from the leader to replication instances (`--instance-health-check-frequency-sec`) 
and the time needed to realize the instance is down (`--instance-down-timeout-sec`). After collecting enough evidence, the leader concludes the main is down and performs failover
using just a handful of RPC messages (correct time depends on the distance between instances). It is important to mention that the whole failover is performed without the loss of committed data
if the newly chosen main (previously replica) had all up-to-date data.

## Raft configuration parameters

Several Raft-related parameters are important for the correct functioning of the cluster. The leader coordinator sends a heartbeat
message to other coordinators every second to determine their health. This configuration option is connected with leader election timeout which
is a randomized value from the interval [2000ms, 4000ms] and which is used by followers to decide when to trigger new election process. Leadership
expiration is set to 2000ms so that cluster can never get into situation where multiple leaders exist. These specific values give a cluster
the ability to survive occasional network hiccups without triggering leadership changes.

<CommunityLinks/>
