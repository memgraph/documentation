---
title: High availability reference queries
description: Complete reference guide for all high availability commands in Memgraph, including cluster management, instance operations, and monitoring queries.
---

import { Callout } from 'nextra/components'
import {CommunityLinks} from '/components/social-card/CommunityLinks'

# High availability reference queries

This page provides a comprehensive reference for all commands available in Memgraph's high availability cluster management.

## Cluster registration commands

<Callout>
**All cluster registration commands (registering coordinators and data instances) should be run on the same coordinator.**
You can pick any coordinator for registering your cluster, which will become the leader coordinator. After cluster has
been set up, the choosing of coordinator does not matter.
</Callout>

### ADD COORDINATOR

Adds a coordinator to the cluster.

```cypher
ADD COORDINATOR coordinatorId WITH CONFIG {
  "bolt_server": boltServer, 
  "coordinator_server": coordinatorServer, 
  "management_server": managementServer
}; 
```

**Parameters:**
- `coordinatorId` (int) - unique integer for each coordinator. You can set a different incrementing integer for each coordinator as you
  register them.
- `boltServer` (string) - Network address in format `"IP_ADDRESS|DNS_NAME:PORT_NUMBER"`. Port is usually set to 7687 as
  that is representative for Bolt protocol. If IPs are ephemeral, it's best to use the DNS name/FQDN. The server IP needs
  to be exposed to the external network, if there are any external applications connected to it.
- `coordinatorServer` (string) - Network address in format `"COORDINATOR_HOSTNAME|COORDINATOR_PORT"`. Coordinator hostname and port
  are set on the command line flags for each coordinator. Ensure coordinator hostname is a DNS name/FQDN if IP addresses are ephemeral.
- `managementServer` (string) - Network address in format `"COORDINATOR_HOSTNAME|MANAGEMENT_PORT"`. Coordinator hostname and management port
  are set on the command line flags for each coordinator. Ensure coordinator hostname is a DNS name/FQDN if IP addresses are ephemeral.


**Implications:**
- The user can choose any coordinator instance to run cluster setup queries. This can be done before or after registering data instances,
the order isn't important.
- `ADD COORDINATOR` query needs to be run for all coordinators in the cluster.
- Bolt server IP needs to be available outside the cluster and not ephemeral.
- Coordinator server and management server IP can be an internal IP/DNS name/FQDN, as the cluster uses it for internal communication.

**Example:**
```cypher
ADD COORDINATOR 1 WITH CONFIG {
  "bolt_server": "my_outside_coordinator_1_IP:7687", 
  "coordinator_server": "memgraph-coordinator-1.default.svc.cluster.local:12000", 
  "management_server": "memgraph-coordinator-1.default.svc.cluster.local:10000"
};
```

### REMOVE COORDINATOR

If during cluster setup or at some later stage of cluster life, the user decides to remove some coordinator instance, 
`REMOVE COORDINATOR` query can be used. This query can only be executed on the leader coordinator to remove follower coordinators.
Current cluster's leader cannot be removed since this is prohibited by NuRaft. In order to remove the current leader,
you first need to trigger leadership change.

```cypher
REMOVE COORDINATOR coordinatorId;
```

**Parameters:**
- `coordinatorId` (integer) - unique integer ID of the coordinator used during the registration

**Example:**
```cypher
REMOVE COORDINATOR 2;
```

### REGISTER INSTANCE

Registers a data instance to the cluster.


```cypher
REGISTER INSTANCE instanceName ( AS ASYNC | AS STRICT_SYNC ) ? WITH CONFIG {
  "bolt_server": boltServer, 
  "management_server": managementServer, 
  "replication_server": replicationServer
};
```

**Parameters:**
- `instanceName` (symbolic name) - unique name of the data instance
- `AS ASYNC` (optional parameter) - register the instance in `ASYNC` replication mode
- `AS STRICT_SYNC` (optional parameter) - register the instance in `STRICT_SYNC` replication mode
- `boltServer` (string) - Network address in format "IP_ADDRESS|DNS_NAME:PORT_NUMBER". Port is usually set to 7687 as
  that is representative for Bolt protocol. If IPs are ephemeral, it's best to use the DNS name/FQDN. The server IP needs
  to be exposed to the external network, if there are any external applications connected to it.
- `managementServer` (string) - ???
- `replicationServer` (string) - ???

**Behaviour:**
- The coordinator instance will connect to the data instance on the `management_server` network address.
- The coordinator instance will start pinging the data instance every `--instance-health-check-frequency-sec` seconds to check its status.
- Data instance will be demoted from main to replica.
- Data instance will start the replication server on `replication_server`.
- This operation will result in writing to the Raft log.

**Implications:**
In case the main instance already exists in the cluster, a replica instance will be automatically connected to the main.
If a replication mode is not specified, REPLICA will be registered in `SYNC` replication mode.
Constructs `( AS ASYNC | AS STRICT_SYNC )` serve to specify a different replication mode other than `SYNC`.

You can only have `STRICT_SYNC` and `ASYNC` or `SYNC` and `ASYNC` replicas together in the cluster. Combining `STRICT_SYNC`
and `SYNC` replicas together doesn't have proper semantic meaning so it is forbidden.

**Example:**
```cypher
REGISTER INSTANCE instance1 WITH CONFIG {
  "bolt_server": "my_outside_instance1_IP:7687", 
  "management_server": "???:10000", 
  "replication_server": "???:20000"
};
```

### UNREGISTER INSTANCE

There are various reasons which could lead to the decision that an instance needs to be removed from the cluster.
The hardware can be broken, network communication could be set up incorrectly, etc. The user can remove the instance
from the cluster using the following query:

```cypher
UNREGISTER INSTANCE instanceName;
```

**Parameters:**
- `instanceName` (symbolic name) - respective name of the data instance

**Implications:**
When unregistering an instance, ensure that the instance being unregistered is
**not** the MAIN instance. Unregistering MAIN can lead to an inconsistent
cluster state. Additionally, the cluster must have an **alive** MAIN instance
during the unregistration process. If no MAIN instance is available, the
operation cannot be guaranteed to succeed.

The instance requested to be unregistered will also be unregistered from the current MAIN's replica set.

**Example:**
```cypher
UNREGISTER INSTANCE instance_1;
```

## Replication role management queries

### SET INSTANCE TO MAIN

Once all data instances are registered, one data instance should be promoted to main.
This can be achieved by using the following query:

```cypher
SET INSTANCE instanceName TO MAIN;
```

**Parameters:**
- `instanceName` (symbolic name) - name of the data instance that is going to be promoted to main

**Behaviour:**
This query will register all other instances as replicas to the new main. 
This operation will result in writing to the Raft log.

**Implications:
If one of the instances is unavailable, setting the instance to MAIN will not succeed.
If there is already a MAIN instance in the cluster, this query will fail.

**Example:**
```cypher
SET INSTANCE instance_0 TO MAIN;
```

### DEMOTE INSTANCE

Demote instance query can be used by an admin to demote the current MAIN to REPLICA.

```cypher
DEMOTE INSTANCE instanceName;
```

**Behaviour:**
- MAIN is demoted to REPLICA
- This operation will result in writing to the Raft log.

**Implications:**
- In this case, the leader coordinator won't perform a failover, but as a user, you should choose promote one of
the data instances to main using the `SET INSTANCE `instance` TO main` query.

<Callout type="info">
By combining the functionalities of queries `DEMOTE INSTANCE instanceName` and `SET INSTANCE instanceName TO main` you get the manual failover capability. This can be useful
e.g during a maintenance work on the instance where the current main is deployed.
</Callout>

**Example:**
```cypher
DEMOTE INSTANCE instance1;
```

## Monitoring commands

### SHOW INSTANCES

You can check the state of the whole cluster using the `SHOW INSTANCES` query.

```cypher
SHOW INSTANCES;
```

**Behaviour:**
The query will display all the Memgraph servers visible in the cluster. With
each server you can see the following information:
 1. Network endpoints they are using for managing cluster state
 2. Health state of server
 3. Role - main, replica, LEADER, FOLLOWER or unknown if not alive
 4. The time passed since the last response time to the leader's health ping 

**Implications:**
This query can be run on either the leader or followers. Since only the leader knows the exact status of the health state
and last response time,  followers will execute actions in this exact order:
  1. Try contacting the leader to get the health state of the cluster, since the leader has all the information. 
  If the leader responds, the follower will return the result as if the `SHOW INSTANCES` query was run on the leader.
  2. When the leader doesn't respond or currently there is no leader, the follower will return all the Memgraph servers
   with the health state set to "down".


### SHOW INSTANCE

You can check the state of the current coordinator to which you are connected by running the following query:

```cypher
SHOW INSTANCE;
```

**Behaviour:**
This query will return the information about:
1. instance name 
2. external bolt server to which you can connect using Memgraph clients 
3. coordinator server over which Raft communication is done 
4. management server which is also used for inter-coordinators communication and 
5. cluster role: whether the coordinator is currently a leader of the follower.

**Implications:**
If the query `ADD COORDINATOR` wasn't run for the current instance, the value of the bolt server will be "".

### SHOW REPLICATION LAG

The user can find the current replication lag on each instance by running `SHOW REPLICATION LAG` on the cluster's leader.
The replication lag is expressed with the number of committed transactions.

```cypher
SHOW REPLICATION LAG;
```

**Implications:**
- Such an info is made durable through snapshots and WALs so restarts won't cause the information loss.
- The information about the replication lag can be useful when manually performing a failover to check whether there is a
risk of a data loss.

## Troubleshooting commands

### FORCE RESET CLUSTER STATE

In case the cluster can't get into a healthy state, or any unexpected event occurs, there is an option to do the force
reset of the cluster.

```cypher
FORCE RESET CLUSTER STATE;
```

**Behaviour:**
1. The coordinator instance will demote each alive instance to replica.
2. From the alive instance it will choose a new main instance.
3. Instances that are down will be demoted to replicas once they come back up.

This operation will result in writing to the Raft log.

**Implications:**
You need to execute a command on the leader coordinator.

<CommunityLinks/>
