---
title: High availability reference queries
description: Complete reference guide for all high availability commands in Memgraph, including cluster management, instance operations, and monitoring queries.
---

import { Callout } from 'nextra/components'
import {CommunityLinks} from '/components/social-card/CommunityLinks'

# High availability reference queries

This page provides a comprehensive reference for all commands available in Memgraph's high availability cluster management.

## User API

### Register instance

Registering instances should be done on a single coordinator. The chosen coordinator will become the cluster's leader.

Register instance query will result in several actions:
1. The coordinator instance will connect to the data instance on the `management_server` network address.
2. The coordinator instance will start pinging the data instance every `--instance-health-check-frequency-sec` seconds to check its status.
3. Data instance will be demoted from main to replica.
4. Data instance will start the replication server on `replication_server`.

```plaintext
REGISTER INSTANCE instanceName ( AS ASYNC | AS STRICT_SYNC ) ? WITH CONFIG {"bolt_server": boltServer, "management_server": managementServer, "replication_server": replicationServer};
```

This operation will result in writing to the Raft log.

In case the main instance already exists in the cluster, a replica instance will be automatically connected to the main. Constructs ( AS ASYNC | AS STRICT_SYNC ) serve to specify
instance's replication mode when the instance behaves as replica. You can only have `STRICT_SYNC` and `ASYNC` or `SYNC` and `ASYNC` replicas together in the cluster. Combining `STRICT_SYNC`
and `SYNC` replicas together doesn't have proper semantic meaning so it is forbidden.


### Add coordinator instance

The user can choose any coordinator instance to run cluster setup queries. This can be done before or after registering data instances,
the order isn't important. 

```plaintext
ADD COORDINATOR coordinatorId WITH CONFIG {"bolt_server": boltServer, "coordinator_server": coordinatorServer}; 
```

<Callout type="info">

`ADD COORDINATOR` query needs to be run for all coordinators in the cluster.

```
ADD COORDINATOR 1 WITH CONFIG {"bolt_server": "127.0.0.1:7691", "coordinator_server": "127.0.0.1:10111", "management_server": "127.0.0.1:12111"};
ADD COORDINATOR 2 WITH CONFIG {"bolt_server": "127.0.0.1:7692", "coordinator_server": "127.0.0.1:10112", "management_server": "127.0.0.1:12112"};
ADD COORDINATOR 3 WITH CONFIG {"bolt_server": "127.0.0.1:7693", "coordinator_server": "127.0.0.1:10113", "management_server": "127.0.0.1:12113"};
```

</Callout>

### Remove coordinator instance

If during cluster setup or at some later stage of cluster life, the user decides to remove some coordinator instance, `REMOVE COORDINATOR` query can be used.
Only on leader can this query be executed in order to remove followers. Current cluster's leader cannot be removed since this is prohibited
by NuRaft. In order to remove the current leader, you first need to trigger leadership change.

```plaintext
REMOVE COORDINATOR <COORDINATOR-ID>;
```


### Set instance to main

Once all data instances are registered, one data instance should be promoted to main. This can be achieved by using the following query:

```plaintext
SET INSTANCE instanceName to main;
```

This query will register all other instances as replicas to the new main. If one of the instances is unavailable, setting the instance to main will not succeed.
If there is already a main instance in the cluster, this query will fail. 

This operation will result in writing to the Raft log.

### Demote instance

Demote instance query can be used by an admin to demote the current main to replica. In this case, the leader coordinator won't perform a failover, but as a user, 
you should choose promote one of the data instances to main using the `SET INSTANCE `instance` TO main` query.

```plaintext
DEMOTE INSTANCE instanceName;
```

This operation will result in writing to the Raft log.

<Callout type="info">

By combining the functionalities of queries `DEMOTE INSTANCE instanceName` and `SET INSTANCE instanceName TO main` you get the manual failover capability. This can be useful
e.g during a maintenance work on the instance where the current main is deployed.

</Callout>


### Unregister instance

There are various reasons which could lead to the decision that an instance needs to be removed from the cluster. The hardware can be broken,
network communication could be set up incorrectly, etc. The user can remove the instance from the cluster using the following query:

```plaintext
UNREGISTER INSTANCE instanceName;
```

When unregistering an instance, ensure that the instance being unregistered is
**not** the main instance. Unregistering main can lead to an inconsistent
cluster state. Additionally, the cluster must have an **alive** main instance
during the unregistration process. If no main instance is available, the
operation cannot be guaranteed to succeed.

The instance requested to be unregistered will also be unregistered from the current main's replica set.

### Force reset cluster state

In case the cluster gets stuck there is an option to do the force reset of the cluster. You need to execute a command on the leader coordinator. 
This command will result in the following actions:

1. The coordinator instance will demote each alive instance to replica.
2. From the alive instance it will choose a new main instance.
3. Instances that are down will be demoted to replicas once they come back up.

```plaintext
FORCE RESET CLUSTER STATE;
```

This operation will result in writing to the Raft log.

### Show instances

You can check the state of the whole cluster using the `SHOW INSTANCES` query. The query will display all the Memgraph servers visible in the cluster. With
each server you can see the following information:
 1. Network endpoints they are using for managing cluster state
 2. Health state of server
 3. Role - main, replica, LEADER, FOLLOWER or unknown if not alive
 4. The time passed since the last response time to the leader's health ping 

This query can be run on either the leader or followers. Since only the leader knows the exact status of the health state and last response time, 
followers will execute actions in this exact order:
  1. Try contacting the leader to get the health state of the cluster, since the leader has all the information. 
  If the leader responds, the follower will return the result as if the `SHOW INSTANCES` query was run on the leader.
  2. When the leader doesn't respond or currently there is no leader, the follower will return all the Memgraph servers
   with the health state set to "down".

```plaintext
SHOW INSTANCES;
```


### Show instance

You can check the state of the current coordinator to which you are connected by running the following query:

```plaintext
SHOW INSTANCE;
```

This query will return the information about:
1. instance name 
2. external bolt server to which you can connect using Memgraph clients 
3. coordinator server over which Raft communication is done 
4. management server which is also used for inter-coordinators communication and 
5. cluster role: whether the coordinator is currently a leader of the follower.

If the query `ADD COORDINATOR` wasn't run for the current instance, the value of the bolt server will be "".

### Show replication lag

The user can find the current replication lag on each instance by running `SHOW REPLICATION LAG` on the cluster's leader. The replication lag is expressed with
the number of committed transactions. Such an info is made durable through snapshots and WALs so restarts won't cause the information loss. The information
about the replication lag can be useful when manually performing a failover to check whether there is a risk of a data loss.

```plaintext
SHOW REPLICATION LAG;
```

<CommunityLinks/>
