---
title: Reference architectures
description: Explore different high availability cluster configurations and deployment scenarios for Memgraph.
---

import { CommunityLinks } from '/components/social-card/CommunityLinks'

# Reference architectures

This page presents different deployment scenarios for Memgraph HA clusters, covering various aspects such
as the number of data centers, geographic distribution, performance scaling, consistency requirements,
and fault tolerance considerations.

## Basic architectures

### Typical HA cluster

A typical Memgraph HA cluster consists of **three data instances and three coordinator instances**.
Data instances store the graph data and require hardware focused on providing sufficient RAM to handle
your workload. Coordinator instances, which manage cluster state, can run on **smaller servers (4GB or
8GB of RAM is sufficient)**.


![](/pages/clustering/high-availability/typical-ha-cluster-no-caption.png)


### Minimal HA cluster

For a minimal HA setup, you can deploy **two data instances (one MAIN and one REPLICA) along with
three coordinator instances**. This requires a **minimum of five instances** to run a high availability
cluster with Memgraph.

![](/pages/clustering/high-availability/minimal-ha-cluster-no-caption.png)

## Architectures for scaling performance

Memgraph is an in-memory graph database that delivers high throughput for both writes and reads on
a standalone instance. While deploying in-memory databases across multiple servers requires more
resources, this trade-off enables the highest performance for real-time use cases.

**Before scaling horizontally or vertically, ensure you have optimized your standalone instance
performance to its limits.**

### Scaling reads in an HA cluster

Scale read queries **horizontally by adding REPLICA instances**. REPLICA instances handle
**read-only queries**, making them ideal for OLAP workloads and other read operations. The
architecture below shows how to increase read capacity by adding additional replica instances.

![](/pages/clustering/high-availability/scaling-reads-with-ha-no-caption.png)

### Scaling writes in an HA cluster

Scale write queries **vertically**. Since **only the MAIN instance accepts writes**, scale it by adding
CPU resources to the server. This approach aligns with how graph databases scale effectively vertically.

While some non-graph databases offer horizontal write scalability, **these solutions are not suitable
for graph traversals, as multi-hop queries would traverse the network and deliver poor performance.**
Some distributed graph solutions offer horizontal sharding, but this comes at the cost of **losing
database or ACID guarantees**â€”trade-offs that most enterprises cannot accept when consistency is critical.

Memgraph's architecture includes **fine-grained locking, lock-free skiplists** for node and relationship
storage, and **MVCC (Multi-Version Concurrency Control)** for data isolation. These features ensure that
**writers don't block readers and vice versa**, meaning vertical scaling delivers the expected performance
gains.

The architecture below shows how to scale data instance servers vertically to achieve the desired write
performance.

![](/pages/clustering/high-availability/scaling-writes-with-ha-no-caption.png)

## Architectures for robust fault-tolerance

### Cross data center cluster

Deploying a cluster across multiple data centers enables **tolerance of entire data center failures**.
The architecture below shows the following topology:
- **Data Center 1**: MAIN instance and one coordinator
- **Data Center 2**: REPLICA instance and one coordinator
- **Data Center 3**: One coordinator

**Failure scenarios:**

- **Data Center 1 fails**: Coordinators maintain quorum (**RAFT tolerates one coordinator failure out
  of three**). **Automatic failover** promotes the REPLICA to MAIN, transferring both write and read load
  to the new MAIN.
- **Data Center 2 fails**: Coordinators maintain quorum with two remaining coordinators. No failover
  occurs since the MAIN is still alive. The REPLICA will **recover automatically** once the data center
  and deployment are restored.
- **Data Center 3 fails**: Coordinators maintain quorum between the remaining two coordinators.

![](/pages/clustering/high-availability/cross-data-center-no-caption.png)

## Other architectures

### Centralized graph with remote REPLICAs

In some scenarios, you may need local copies of data to meet performance SLAs that require minimal
latency. To achieve this, deploy REPLICAs in local data centers.

The architecture below shows a setup with `ASYNC` replicas in remote locations. Set the
`sync_failover_only` flag to `true` since writes only go to the centralized graph to consolidate
information (**failover cannot occur to the local replica**).

![](/pages/clustering/high-availability/centralised-main-with-remote-instances-no-caption.png)

The next architecture makes this topology more robust by adding a `SYNC` or `STRICT_SYNC` replica at
the central location. This ensures failover capability if the MAIN fails, as **failover can only
happen between sync instances**.

![](/pages/clustering/high-availability/centralised-main-with-remote-instances-with-failover-no-caption.png)

<CommunityLinks />
