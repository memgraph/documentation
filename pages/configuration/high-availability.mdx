---
title: High availability
description: Dive into the documentation page for Memgraph and learn how to configure a cluster of Memgraph instances to achieve high availability.
---

import { Callout } from 'nextra/components'

# High availability (Enterprise)

<Callout type="warning">

Memgraph 2.15 introduced a high availability enterprise feature, which is only enabled if used with `--experimental-enabled=high-availability` flag.

</Callout>


A cluster is considered highly available if, at any point, there is some instance that can respond to a user query. Our high availability 
relies on replication. The cluster consists of:
- The MAIN instance on which the user can execute write queries
- The REPLICA instances that can only respond to read queries and coordinators that manage the cluster state. 

<Callout type="info">

In the Memgraph 2.15 release, the cluster is highly available
in the sense that there will always be a consistent MAIN instance on which the user can write, but the coordinator is implemented only as a single
instance for which it is assumed that it cannot fail. Work is being done on introducing NuRaft to connect coordinator instances to a Raft cluster
so that at any point, there is an active coordinator that can correctly manage cluster state.

</Callout>


## Cluster management

You can start the coordinator instance by specifying `--raft-server-id` and `--raft-server-port` flags. The coordinator instance only responds to
queries related to high availability, so you cannot execute any data-oriented query on it. The Raft server port is used for the Raft protocol, which
all coordinators use to ensure the consistency of the cluster's state. The replication instance is distinguished from the coordinator instance by
specifying `--coordinator-server-port` flag. This port is used for RPC network communication that happens between coordinators and replication
instances. When started by default replication instance is MAIN. All replication instances should be started with flag `--restore-replication-state-at-startup`
so that its role (MAIN or REPLICA) is restored on restart. The coordinator will ensure that no data inconsistency can happen during and after the instance's
restart. Currently, all REPLICAs should be SYNC replicas to avoid branching from happening.
Once all instances are started, the user can start registering replication instances.

<Callout type="info">

The Raft consensus algorithm ensures that all nodes in a distributed system
agree on a single source of truth, even in the presence of failures, by electing
a leader to manage a replicated log. It simplifies the management of the
replicated log across the cluster, providing a way to achieve consistency and
coordination in a fault-tolerant manner.

</Callout>

### Register instance

Register instance query will result in several actions:
1. The coordinator instance will connect to the replication instance 
2. The coordinator instance will start pinging the replication instance every `--instance-health-check-frequency-sec` seconds to check 
the status of the replication instance.
3. Replication instance will be demoted from MAIN to REPLICA.
4. Replication instance will start the replication server on `replicationSocketAddress` specified in the query to be in sync with MAIN.

Both the coordinator and replication socket addresses need to be unique.

```plaintext
REGISTER INSTANCE instanceName ON coordinatorSocketAddress ( AS ASYNC ) ? WITH replicationSocketAddress ;
```

### Set instance to MAIN

Once all instances are registered, one replication instance should be promoted to MAIN. This can be achieved by using the following query:

```plaintext
SET INSTANCE instanceName to MAIN;
```

This query will register all other instances as REPLICAs to the new MAIN. If one of the instances is unavailable, setting the instance to MAIN will not succeed.

If there is already a MAIN instance in the cluster, this query will fail. 


### Unregister instance

There are various reasons which could lead to the decision that an instance needs to be removed from the cluster. The hardware can be broken,
network communication could be set up incorrectly, etc. The user can remove the instance from the cluster using the following query:

```plaintext
UNREGISTER INSTANCE instanceName;
```

At the moment of registration, the instance which you want to unregister must not be MAIN because unregistering MAIN could lead to the cluster which
is in an inconsistent state. 

The instance requested to be unregistered will also be unregistered from the current MAIN's REPLICA set.

### Show instances

You can check the state of the whole cluster using `SHOW INSTANCES` query. The query will show what instances are visible in the cluster,
which Raft port are they using (single coordinator for now), which coordinator port are they using, and are they considered alive from the coordinator's
perspective and their role (are they MAIN, REPLICA, coordinator or unknown if not alive).

```plaintext
SHOW INSTANCES;
```
## Setting config for highly-available cluster

There are several flags that you can use for managing the cluster. Flag `--coordinator-server-port` is used for distinguishing replication instances
from coordinators. The provided flag needs to be unique. Setting a flag will create an RPC server on replication instances capable of 
responding to the coordinator's RPC messages. 

<Callout type="info">

RPC (Remote Procedure Call) is a protocol for executing functions on a remote
system. RPC enables direct communication in distributed systems and is crucial
for replication tasks.

</Callout>

Flags `--raft-server-id` and `--raft-server-port` need to be unique and specified on coordinator instances. They will cause the creation of a Raft 
server that coordinator instances use for communication. Flag `--instance-health-check-frequency-sec` specifies how often should leader coordinator
check the status of the replication instance to update its status. Flag `--instance-down-timeout-sec` gives the user the ability to control how much time should
pass before the coordinator starts considering the instance to be down. 

<Callout type="info">

Consider the instance to be down only if several consecutive pings fail because a single ping can fail because of a large number of different reasons. 

</Callout>

There is an additional flag, `--instance-get-uuid-frequency-sec`, which sets 
how often the coordinator should check on REPLICA instances if it follows the correct MAIN instance. REPLICA may die and get back up
before the coordinator notices that. In that case, REPLICA will not  follow MAIN for `--instance-get-uuid-frequency-sec` seconds. It is advisable to set 
the flag to more than `--instance-down-timeout-sec`, as it is needed first to confirm the instance was down and not that there was some networking issue.


## Failover

### Single coordinator
Currently, failover is operated from the point of a single coordinator. The coordinator is not supposed to fail. If the coordinator fails best solution
is to register each instance again and set the old instance, which was MAIN as MAIN.


### Failover procedure
Every `--instance-health-check-frequency-sec` seconds, the coordinator contacts each instance. 
The instance is not considered to be down for `--instance-down-timeout-sec`. Expectation currently is to set `--instance-health-check-frequency-sec`
to be less than `--instance-down-timeout-sec` and for `--instance-down-timeout-sec to be multiplier` of `--instance-health-check-frequency-sec` with coefficiient N.
Set the multiplier coefficient to be N>=2.

Once the instance is down, in case it is REPLICA coordinator will try to contact it again every `--instance-health-check-frequency-sec`. REPLICA
always returns as REPLICA. MAIN can return as MAIN if failover doesn't succeed in the meantime or if it comes back up faster
than `--instance-down-timeout-sec` timeout, which is set.

If the instance is MAIN, at that point, failover starts. From alive REPLICAs coordinator chooses a new potential MAIN. 
The coordinator first contacts each alive REPLICA to stop listening to the old MAIN and start
listening to the potential new MAIN. Once each alive REPLICA acknowledges that it stopped listening to the old MAIN,
the coordinator sends an RPC request to the potential new MAIN, which is still in REPLICA state, to promote itself to the MAIN instance and
sends all instances from the cluster to which the new MAIN will replicate data. If the instance is down at the moment, the new MAIN will establish 
a connection later when it comes back up and recover it to the correct state.
In RPC, request new potential MAIN gets UUID which REPLICAs are ready to listen to.

Once the old MAIN gets back up, an RPC request is sent to demote MAIN to REPLICA, and a new RPC request is sent for REPLICA to start listening to 
the new MAIN.

The coordinator tracks at all times which instance was the last MAIN. 

### Old MAIN getting back to the cluster
If, at a certain point new MAIN dies while the old MAIN is still not demoted to REPLICA, both instances might get back up as MAIN.
Old MAIN will get demoted to REPLICA, while new MAIN will stay as MAIN or also be demoted to REPLICA, depending on fact whether 
failover has succeeded or not.

### Demoting MAIN to REPLICA
When MAIN is demoted to REPLICA it can happen that MAIN gets demoted, but setting now new REPLICA to listen to the current MAIN fails. If
that happens, the coordinator will retry every `--instance-health-check-frequency-sec` to set up REPLICA to start listening to the current MAIN.


### Replication concerns

If at some point it happens that MAIN goes down, but REPLICA, which is chosen as the new MAIN, has less up-to-date data than other 
REPLICAs, failover won't succeed as from MAINs perspective it cannot become new MAIN as it has some other instance which is more up to date
and therefore diverged from the new MAIN.
At that point, the best solution is to UNREGISTER the instance that has less up-to-date data, let failover succeed and set the instance back in the cluster.
Another option is to drop data from an instance that is more up-to-date by shutting it down and deleting snapshots and WAL directories in 
`--data-directory`.

## Instances' restart

Memgraph operates under the assumption that the existing single coordinator instance cannot fail. Replication instances can fail both as MAIN and as REPLICA.
When an instance that was REPLICA comes back, it won't accept updates from any instance until the coordinator updates its responsible peer. This should
happen automatically when the coordinator's ping to the instance passes. When the MAIN instance comes back, any writing to the MAIN instance will be forbidden 
until a ping from the coordinator passes. When the coordinator realizes the once-MAIN instance is alive, it will choose between enabling writing on old MAIN
or demoting it to REPLICA. The choice depends on whether there was any other MAIN instance chosen between the old MAIN's failure and restart.

Since the instance can die and get back up use `--replication-state-at-startup=true` to restore the last replication state. This configuration will restore the instance to
the state it was last, and everything should function normally from that point on. If this flag is not set, every instance will restart as MAIN and that case is not handled.

## Handling errors

Distributed systems can fail in various ways. The logic is implemented in a way that the Memgraph instance will be resilient to occasional network
failures and independent machine failures. That's why there are parameters controlling the frequency of health checks from coordinator to replication
instances and the time needed to realize the instance is down.

## Example

This example will show how to set up a highly available cluster in Memgraph using a single coordinator.

### 1. Start all instances

i.) Start coordinator:
```plaintext
docker run  --name coord1 -p 7687:7687 -p 7444:7444 memgraph/memgraph-mage --bolt-port=7687 --log-level=TRACE --data-directory=/tmp/mg_data_coord1 --log-file=/tmp/coord1.log --also-log-to-stderr --raft-server-id=1 --raft-server-port=10111 --experimental-enabled=high-availability
```

ii.) Start instance1:
```plaintext
docker run  --name instance1 -p 7688:7688 -p 7445:7444 memgraph/memgraph-mage --bolt-port=7688 --log-level=TRACE --data-directory=/tmp/mg_data_instance1 --log-file=/tmp/instance1.log --also-log-to-stderr --replication-restore-state-on-startup --coordinator-server-port=10011 --experimental-enabled=high-availability
```

iii.) Start instance2:
```plaintext
docker run --name instance1 -p 7689:7689 -p 7446:7444 memgraph/memgraph-mage --bolt-port=7689 --log-level=TRACE --data-directory=/tmp/mg_data_instance2 --log-file=/tmp/instance2.log --also-log-to-stderr --replication-restore-state-on-startup --coordinator-server-port=10012 --experimental-enabled=high-availability
```

iv.) Start instance3:
```plaintext
docker run --name instance1 -p 7690:7690  -p 7447:7444 memgraph/memgraph-mage --bolt-port=7690 --log-level=TRACE --data-directory=/tmp/mg_data_instance3 --log-file=/tmp/instance3.log --also-log-to-stderr --replication-restore-state-on-startup --coordinator-server-port=10013 --experimental-enabled=high-availability
```

### 2. Register instances

i.) Start communication with the coordinator with any Memgraph client on port 7687.
```plaintext
mgconsole --port=7687
```
ii.) Register all 3 instances as part of the cluster:

<Callout type="info">

Replace `<ip_address>` with the container's IP address. This is necessary for Docker deployments where instances are not on the local host.

</Callout>

```plaintext
REGISTER INSTANCE instance_1 ON '<ip_address_1>:10011' WITH '127.0.0.1:10001';
REGISTER INSTANCE instance_2 ON '<ip_address_2>:10012' WITH '127.0.0.1:10002';
REGISTER INSTANCE instance_3 ON '<ip_address_3>:10013' WITH '127.0.0.1:10003';
```

iii.) Set instance_3 as MAIN:
```plaintext
SET INSTANCE instance_3 TO MAIN;
```

iv.) Connect to the coordinator and check cluster state with `SHOW INSTANCES`;

| instance_name | raft_socket_address | coordinator_socket_address | alive | role        |
| ------------- | ------------------- | -------------------------- | ----- | ----------- |
| coordinator_1 | 127.0.0.1:10111     | ""                         | True  | coordinator |
| instance_1    | ""                  | 127.0.0.1:10011            | True  | replica     |
| instance_2    | ""                  | 127.0.0.1:10012            | True  | replica     |
| instance_3    | ""                  | 127.0.0.1:10013            | True  | main        |


### 3. Check automatic failover

Let's say that the current MAIN instance is down for some reason. After `--instance-down-timeout-sec` seconds, the coordinator will realize
that and automatically promote the first alive REPLICA to become the new MAIN. The output of running `SHOW INSTANCES` could then look like:

| instance_name | raft_socket_address | coordinator_socket_address | alive | role        |
| ------------- | ------------------- | -------------------------- | ----- | ----------- |
| coordinator_1 | 127.0.0.1:10111     | ""                         | True  | coordinator |
| instance_1    | ""                  | 127.0.0.1:10011            | True  | main        |
| instance_2    | ""                  | 127.0.0.1:10012            | True  | replica     |
| instance_3    | ""                  | 127.0.0.1:10013            | False | unknown     |
