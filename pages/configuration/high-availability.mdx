---
title: High availability
description: Dive into the documentation page for Memgraph and learn how to configure cluster of Memgraph instances to achive high availability.
---

import { Callout } from 'nextra/components'

# High availability (Enterprise)

<Callout type="warning">

Memgraph 2.15 introduced high availability enterprise feature which is only enabled if used with `--experimental-enabled=high-availability' flag.

</Callout>


A cluster is considered highly available if, at any point, there is some instance that can respond to a user query. Our high availability 
relies on replication. The cluster consists of:
- The MAIN instance on which the user can execute write queries
- The REPLICA instances that can only respond to read queries and coordinators that manage the cluster state. 

<Callout type="info">

In the Memgraog 2.15 release, the cluster is highly available
in the sense that there will always be a consistent MAIN instance on which the user can write but the coordinator is implemented only as a single
instance for which we assume that it cannot fail. We are working on introducing NuRaft to connect coordinator instances to a Raft cluster
so that at any point, there is an active coordinator that can correctly manage cluster state.

</Callout>


## Cluster management

You can start coordinator instance by specifying `--raft-server-id` and `--raft-server-port` flags. Coordinator instance only responds to
queries related to high availability so you cannot execute any data-oriented query on it. Raft server port is used for RAFT protocol which
all coordinators are using to ensure consistenty of cluster's state. Replication instance is distinguished from coordinator instance by
specifying `--coordinator-server-port` flag. This port is used for RPC network communication that happens between coordinators and replication
instance. When started by default replication instance is MAIN. All replication instances should be started with flag `--restore-replication-state-at-startup`
so that its role (MAIN or REPLICA) is restored on restart. Coordinator will ensure that no data inconsistency can happen during and after instance's
restart. Currently all replicas should be SYNC replicas to avoid branching from happening. We plan to add support for ASYNC replicas in 2.16.
Once all instances are started, the user can start registering replication instances.

### Register instance

Register instance query will result in several actions:
1. Coordinator instance will connect to replication instance 
2. Coordinator instance will start pinging replication instance every `--instance-health-check-frequency-sec` seconds to check 
the status of replication instance.
3. Replication instance will be demoted from MAIN to REPLICA.
4. Replication instance will by default by REPLICA in sync with MAIN which will start replication server on replicationSocketAddress specified in the query.

Both coordinator and replication socket address need to be unique.

```plaintext
REGISTER INSTANCE instanceName ON coordinatorSocketAddress ( AS ASYNC ) ? WITH replicationSocketAddress ;
```

### Set instance to MAIN

Once all instances are registered, one replication instance should be promoted to MAIN. This can be achieved by using following query:

```plaintext
SET INSTANCE instanceName to MAIN;
```

This query will register all other instances as replicas to the new MAIN. If one of the instances is unavailable, setting instance to MAIN will not succeed.

If there is already MAIN instance in the cluster, this query will fail. 


### Unregister instance

There are various reasons which could lead to the decision that an instance needs to be removed from the cluster. The hardware can be broken,
network communication could be wrongly setup etc. The user can remove instance from the cluster using following query:

```plaintext
UNREGISTER INSTANCE instanceName;
```

At the moment of registration, the instance which you want to unregister must not be MAIN because unregistering MAIN could lead to cluster which
is in inconsistent state. 

The instance requested to be unregistered will also be unregistered from the current MAIN's REPLICA set.

### Show instances

You can check the state of the whole cluster using `SHOW INSTANCES` query. The query will show what instances are visible in the cluster,
which raft port are they using (single coordinator for now), which coordinator port are they using, are they considered alive from coordinator's
perspective and which role do they have (are they MAIN, REPLICA, coordinator or unknown if not alive).

```plaintext
SHOW INSTANCES;
```
##  


## Setting config for highly-available cluster

Memgraph's 2.15 release added several flags to user for managing cluster. Flag `--coordinator-server-port` is used for distinguishing replication instances
from coordinators. The provided flag needs to be unique. Setting flag will result in creating RPC server on replication instances capable of 
responding to coordinator's RPC messages. 
Flags `--raft-server-id` and `--raft-server-port` need to be unique and specified on coordinator instances. They will cause creation of RAFT 
server used for communication of coordinator instances. Flag `--instance-health-check-frequency-sec` specifies how often should leader coordinator
check status of replication instance to update its status. Flag `--instance-down-timeout-sec` gives user the ability to control how much time should
pass before coordinator starts considering instance to be down. We advise user to consider instance to be down only if several consecutive pings
failed because a single ping can fail because of a million reasons. There is additional flag `--instance-get-uuid-frequency-sec` which sets 
how often should coordinator check on REPLICA instances if it follows correct MAIN instance. It can happen that REPLICA dies and gets back up,
before coordinator notices that. In that case REPLICA will not  follow MAIN for `--instance-get-uuid-frequency-sec` seconds. We advise to set 
the flag to more than `--instance-down-timeout-sec`, as we should first confirm instance was down, and not that we had some networking issue.


## Achieving high availability

What is the core operation? -> Automatic failover.


## Failover

### Single coordinator
Currently failover is operated from point of single coordinator. Coordinator is not suppose to fail. If coordinator fails best solution
is to register each instance again, and set old instance which was MAIN as MAIN.


### Failover procedure
Every `--instance-health-check-frequency-sec` seconds, coordinator contacts each instance. 
Instance is not considered to be down for `--instance-down-timeout-sec`. Expectation currently is to set `--instance-health-check-frequency-sec`
to be less than `--instance-down-timeout-sec` and for `--instance-down-timeout-sec to be multiplier` of `--instance-health-check-frequency-sec` with coefficiient N.
We suggest for multiplier coefficient to be N>=2.

Once instance is down, in case it is REPLICA coordinator will try to contact it again every `--instance-health-check-frequency-sec`. REPLICA
always returns as REPLICA. MAIN can return as MAIN if failover doesn't succeed in the meantime or if it comes back up faster
than `--instance-down-timeout-sec` timeout which is set.

If instance is MAIN, at that point failover starts. From alive replicas coordinator chooses new potential MAIN. 
Coordinator first contacts each REPLICA which is alive to stop listening to the old MAIN and to start
listening to the potential new MAIN. Once each alive REPLICA acknowlages that it stopped listening to the old MAIN,
coordinator sends RPC request to the potential new MAIN which is still in REPLICA state, to promote itself to MAIN instance and
sends all instances from cluster to which new MAIN will replicate data. If instance is down at the moment, new MAIN will establish 
connection at later point when it comes back up, and recover it to correct state.
In RPC request new potential MAIN gets UUID which replicas are ready to listen to.

Once old MAIN gets back up, RPC request is sent to demote MAIN to REPLICA, and new RPC request is sent for REPLICA to start listening to 
the new MAIN.

Coordinator tracks at all times which instance was last MAIN. 

### Old MAIN getting back to cluster
If at certain point new MAIN dies while old MAIN is still not demoted to REPLICA, it could happen that both instances get back up as MAIN.
Old MAIN will get demoted to REPLICA, while new MAIN will stay as MAIN or also be demoted to REPLICA, depending on fact whether 
failover has suceeded or not.

### Demoting MAIN to REPLICA
When MAIN is demoted to REPLICA it can happen that MAIN gets demoted, but setting now new REPLICA to listen to the current MAIN fails. If
that happens, coordinator will retry every `--instance-health-check-frequency-sec` to set up REPLICA to start listening to the current MAIN.


### Replication concerns

If at some point it happens that MAIN goes down, but REPLICA which is chosen as new MAIN has less up to date data than other 
REPLICAs, failover won't succeed as from MAINs perspective it cannot become new MAIN as it has some other instance which is more up to date
and therefore diverged from new MAIN.
At that point, best solution is to UNREGISTER instance which has less up to date data, let failover succeed and set instance back in cluster.
Other option is to drop data from instance which is more up to date, by shutting it down and deleting snapshots and WAL directories in 
`--data-directory`.


MAIN

3. Failover logic:
- single-coordinator
- whole logic, callbackovi kako to funkcionira


- diverged from main -> user has to manually solve it if happens
- idempotentno demotanje u repliku 


- REPLICA can talk only with a single main, possible that cannot talk with any instance for a short time.. -> to avoid inconsitencies :checmark:
- choosing REPLICA currently simple :Checkmark: 


## Instances' restart


In 2.15 release, we assume there is a single coordinator instance which cannot fail. Replication instance can fail both as MAIN and as REPLICA.
When instance which was REPLICA comes back, it won't accept updates from any instance until coordinator updates its responsible peer. This should
happen automatically when coordinator's ping to the instance passes. When MAIN instance comes back, any writing to the MAIN instance will be forbidden 
until ping from coordinator passes. When the coordinator realizes that the once-MAIN instance is alive, it will choose between enabling writing on old MAIN
or demoting it to REPLICA. The choice depends on whether there was any other MAIN instance chosen between old MAIN's failure and restart.

Since instance can die and get back up, use `--replication-state-at-startup=true` to restore last replication state. With this config we will restore instance in 
state it was last and everything should function normally from that point. In case this flag is not set, every instance will restart as MAIN and that case is not handled.

## Handling errors

Distributed system can fail in various ways. The logic is implemented in a way that Memgraph instance will be resilient to occasional network
failures and independent machine failures. That's why there are parameters controlling frequency of health checks from coordinator to replication
instances and the time needed to realize the instance is down.

## Example

This example will show how to setup highly-available cluster in Memgraph using a single coordinator.

### 1. Start all instances

i.) Start coordinator:
```plaintext
docker run  --name coord1 --rm -p 7687:7687 -p 7444:7444 memgraph/memgraph-mage --bolt-port=7687 --log-level=TRACE --data-directory=/tmp/mg_data_coord1 --log-file=/tmp/coord1.log --also-log-to-stderr --raft-server-id=1 --raft-server-port=10111 --experimental-enabled=high-availability
```

ii.) Start instance1:
```plaintext
docker run  --name instance1 --rm -p 7688:7688 -p 7445:7444 memgraph/memgraph-mage --bolt-port=7688 --log-level=TRACE --data-directory=/tmp/mg_data_instance1 --log-file=/tmp/instance1.log --also-log-to-stderr --replication-restore-state-on-startup --coordinator-server-port=10011 --experimental-enabled=high-availability
```

iii.) Start instance2:
```plaintext
docker run --name instance1 --rm -p 7689:7689 -p 7446:7444 memgraph/memgraph-mage --bolt-port=7689 --log-level=TRACE --data-directory=/tmp/mg_data_instance2 --log-file=/tmp/instance2.log --also-log-to-stderr --replication-restore-state-on-startup --coordinator-server-port=10012 --experimental-enabled=high-availability
```

iv.) Start instance3:
```plaintext
docker run --name instance1 --rm -p 7690:7690  -p 7447:7444 memgraph/memgraph-mage --bolt-port=7690 --log-level=TRACE --data-directory=/tmp/mg_data_instance3 --log-file=/tmp/instance3.log --also-log-to-stderr --replication-restore-state-on-startup --coordinator-server-port=10013 --experimental-enabled=high-availability
```

### 2. Register instances

i.) Start communication with the coordinator with any Memgraph client on port 7687.
```plaintext
mgconsole --port=7687
```
ii.) Register all 3 instances as part of the cluster:
```plaintext
REGISTER INSTANCE instance_1 ON '127.0.0.1:10011' WITH '127.0.0.1:10001';
REGISTER INSTANCE instance_2 ON '127.0.0.1:10012' WITH '127.0.0.1:10002';
REGISTER INSTANCE instance_3 ON '127.0.0.1:10013' WITH '127.0.0.1:10003';
```

iii.) Set instance_3 as MAIN:
```plaintext
SET INSTANCE instance_3 TO MAIN;
```

iv.) Connect to coordinator and check cluster state with `SHOW INSTANCES`;

| instance_name | raft_socket_address | coordinator_socket_address | alive | role        |
| ------------- | ------------------- | -------------------------- | ----- | ----------- |
| coordinator_1 | 127.0.0.1:10111     | ""                         | True  | coordinator |
| instance_1    | ""                  | 127.0.0.1:10011            | True  | replica     |
| instance_2    | ""                  | 127.0.0.1:10012            | True  | replica     |
| instance_3    | ""                  | 127.0.0.1:10013            | True  | main        |


### 3. Check automatic failover

Let's say that the current MAIN instance is down because of some reason. After `--instance-down-timeout-sec` seconds, coordinator will realize
that and automatically promote first alive REPLICA to become new MAIN. The output of running `SHOW INSTANCES` could then look like:

| instance_name | raft_socket_address | coordinator_socket_address | alive | role        |
| ------------- | ------------------- | -------------------------- | ----- | ----------- |
| coordinator_1 | 127.0.0.1:10111     | ""                         | True  | coordinator |
| instance_1    | ""                  | 127.0.0.1:10011            | True  | main        |
| instance_2    | ""                  | 127.0.0.1:10012            | True  | replica     |
| instance_3    | ""                  | 127.0.0.1:10013            | False | unknown     |
