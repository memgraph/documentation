---
title: GraphChat
description: GraphChat is a feature within Memgraph Lab that allows users to query the Memgraph database using the English language instead of Cypher queries.
---

import { Callout } from 'nextra/components'
import {CommunityLinks} from '/components/social-card/CommunityLinks'

# GraphChat

**GraphChat** is a natural language querying tool integrated into Memgraph Lab,
designed to transform how users interact with graph databases. It translates
English queries into Cypher and it’s designed for non-technical users while
still catering to advanced developers.

By using Large Language Models (LLMs), such as OpenAI’s GPT-4, it translates
natural language queries into Cypher commands, delivering precise and actionable
results. 

GraphChat is particularly beneficial for teams seeking faster insights, easier
data exploration, and a more collaborative approach to graph database
management. Whether you're a data scientist exploring complex relationships or a
business analyst seeking quick answers, GraphChat simplifies the querying
process while maintaining its technical depth.

![graphchat](/pages/data-visualization/graphchat.png)

## Features

1. **Natural language to Cypher translation**: Query the database without needing to learn Cypher.

2. **Error handling and auto-retry**: Automatically retries invalid Cypher queries by retrying up to three times (user-configurable).

3. **Integration with multiple LLM providers**:
   - **OpenAI**: Use OpenAI’s GPT models with API key authentication.
   - **Azure OpenAI**: Supports enterprise-grade LLM usage with Azure.
   - **Ollama**: Enables local LLM integration for enhanced privacy.

4. **Threaded conversations**: Create multiple threads for distinct topics or
  comparisons (e.g., comparing outputs from different models). This helps keep
  discussions organized.

5. **Conversation history context**:
   - **Default**: Last five messages for context-aware queries.
   - **Customizable**: Users can define history length or exclude specific conversations.

6. **Detailed customization**:
   - **Supports various LLMs**: (e.g., GPT-4, GPT-4o).
   - **Adjustable parameters**: Temperature, retries, conversation depth, etc.
   - **Flexible Configuration**: Configure hundreds of models for different tasks.


## Setup Guide

Detailed instructions to get users started with GraphChat.

### Prerequisites

1. Ensure you have Memgraph [installed](/getting-started/install-memgraph).
2. You have prepared and [imported your data](/data-migration) into Memgraph.
3. You have LLM provider credentials.

<Callout type="info">
From Memgraph 2.16, GraphChat doesn't require MAGE installed. For schema
information, GraphChat uses [`SHOW SCHEMA INFO`
query](/querying/schema#run-time-schema-tracking), if available. If the `SHOW
SCHEMA INFO` query is not enabled it will try using the [schema-related
procedures](/querying/schema#schema-related-procedures). If none of the above
works, it will run Cypher queries.
</Callout>

### OpenAI

Use OpenAI’s models for processing natural language queries. Set up a connection to OpenAI by providing a valid OpenAPI key connection.

### Azure OpenAI

Set up a connection to Azure OpenAI by providing:

- `azureOpenApiVersion`: Your Azure OpenAI service version. [Find the list of versions here](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions).
- `azureOpenApiApiKey`: Your Azure OpenAI API key
- `azureOpenApiInstanceName`: Your Azure OpenAI instance name
- `azureOpenApiDeploymentName`: Your Azure OpenAI deployment name.

Additional Azure OpenAI integration details can be found in the [Azure OpenAI documentation](https://js.langchain.com/docs/integrations/text_embedding/azure_openai).

### Ollama

For local LLM model setup, you can use Ollama:

- Provide the local endpoint URL, such as `http://localhost:11434`.

<Callout>

If you are having issues connecting to Ollama, try using `host.docker.internal`
instead of `localhost` or `127.0.0.1`. Additional settings may be required if
you are using
[Docker](/getting-started/install-memgraph/docker#issues-when-connecting-to-memgraph-lab-to-memgraph)
or [Docker Compose](/getting-started/install-memgraph/docker-compose) to run
Memgraph and Memgraph Lab.

</Callout>

Learn more about Ollama and how to set it up for local LLM model use:

- [Ollama Home Page](https://ollama.com/)
- [Ollama GitHub Repository](https://github.com/ollama/ollama)
- [Ollama Docker Hub](https://hub.docker.com/r/ollama/ollama)

Ensure you follow the appropriate guidelines and documentation when setting up these connections to take full advantage of the GraphChat capabilities within Memgraph Lab.

## Resources

- [Talking to Your Graph Database with LLMs Using GraphChat](https://memgraph.com/blog/talking-to-graph-database-llms-graphchat-webinar-recap)
- [From Questions to Queries: How to Talk to Your Graph Database With LLMs?](https://memgraph.com/webinars/how-to-talk-to-your-graph-database-with-llm)

<CommunityLinks/>
